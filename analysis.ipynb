{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS6795_Final_Project_Sentiment_Analysis.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NxD9ooVYlIcG"},"source":["# Import"]},{"cell_type":"code","metadata":{"id":"hxl4UrYLoIY6","executionInfo":{"status":"ok","timestamp":1619491748416,"user_tz":240,"elapsed":4032,"user":{"displayName":"Lastk7","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz2ulFWgLLy_9S6Hxq2-LMJAKNqCtmNEvz6Hp3=s64","userId":"16520756898672502912"}}},"source":["import os\n","import re\n","import io\n","import time\n","import math\n","import pickle\n","from collections import Counter\n","from argparse import Namespace\n","\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n","from torch.utils.data import DataLoader\n","\n","import torchtext\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import Vocab\n","\n","from tqdm.notebook import tqdm\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from google.colab import drive, files\n","\n","import nltk\n","from nltk.corpus import stopwords\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n","\n","from mlxtend.plotting import plot_confusion_matrix"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FA-l2Edhlbvz"},"source":["# Settings"]},{"cell_type":"code","metadata":{"id":"JGs8MT4Ile_y","executionInfo":{"status":"ok","timestamp":1619491749119,"user_tz":240,"elapsed":694,"user":{"displayName":"Lastk7","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz2ulFWgLLy_9S6Hxq2-LMJAKNqCtmNEvz6Hp3=s64","userId":"16520756898672502912"}}},"source":["DATA_FOLDER_PATH = '/content/Gdrive/MyDrive/CS 6795 Cog-Sci Final Project/data/'\n","\n","args = Namespace(\n","    sample_size=300000, # the original dataset is too big to be processed efficiently \n","\n","    test_ratio=0.1,\n","    val_ratio=0.2,\n","    \n","    model_name='baseline_lstm_with_dropout',\n","    num_epoch=7,\n","    gradient_clip=1.0,\n","    learning_rate=1e-3,\n","    criterion='crossEntropy',\n","    optimizer='Adam',\n","\n","    num_layers=2,\n","    dropout=0.3,\n","    embedding_dim=300,\n","    hidden_dim=300,\n","    output_dim=2, # output_dim is the number of different sentiments in the target dataset\n","\n","    batch_size=128,\n","    batch_first=False,\n","\n","    train_csv_path=DATA_FOLDER_PATH + 'training_1_6_million.csv',\n","    # pretrained_baseline_lstm_with_dropout_path=DATA_FOLDER_PATH + 'baseline_lstm_with_dropout.pt',\n","\n","    load_pretrained_weight=True,\n","    save_pretrained_model=False,\n","    read_vocab=True,\n","    \n","    device=torch.device('cuda' if torch.cuda.is_available else 'cpu'),\n",")"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9sDvnJVUlLkd"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zCkliKD-or07","executionInfo":{"status":"ok","timestamp":1619491775701,"user_tz":240,"elapsed":22926,"user":{"displayName":"Lastk7","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz2ulFWgLLy_9S6Hxq2-LMJAKNqCtmNEvz6Hp3=s64","userId":"16520756898672502912"}},"outputId":"734464ae-0243-47aa-8444-52aa7a21f652"},"source":["drive.mount('/content/Gdrive')\n","large_data_df = pd.read_csv(args.train_csv_path, encoding='latin').sample(args.sample_size)\n","# rename the columns into standardized names\n","large_data_df.columns = ['sentiment_score', 'id', 'date', 'status', 'username', 'tweet']\n","# convert the sentiment score of 4 into 1\n","large_data_df.loc[large_data_df['sentiment_score'] == 4, 'sentiment_score'] = 1"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/Gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"BDXgfMD4ojTH","executionInfo":{"status":"ok","timestamp":1619491776977,"user_tz":240,"elapsed":415,"user":{"displayName":"Lastk7","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz2ulFWgLLy_9S6Hxq2-LMJAKNqCtmNEvz6Hp3=s64","userId":"16520756898672502912"}},"outputId":"e5244e20-4c80-47ee-ebec-6061c59b18db"},"source":["large_data_df.head()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentiment_score</th>\n","      <th>id</th>\n","      <th>date</th>\n","      <th>status</th>\n","      <th>username</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3885</th>\n","      <td>0</td>\n","      <td>1468696840</td>\n","      <td>Tue Apr 07 03:13:26 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>KatelynWelch</td>\n","      <td>ooooooooooooh my headddd  uncle johnny i never...</td>\n","    </tr>\n","    <tr>\n","      <th>12255</th>\n","      <td>0</td>\n","      <td>1551735091</td>\n","      <td>Sat Apr 18 09:50:17 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>Gita</td>\n","      <td>what a shame! you can't type persian &amp;amp; rea...</td>\n","    </tr>\n","    <tr>\n","      <th>133516</th>\n","      <td>0</td>\n","      <td>1836020829</td>\n","      <td>Mon May 18 07:13:03 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>FieFieSoMajor</td>\n","      <td>@rugzdbewler hardly working lol. I think I got...</td>\n","    </tr>\n","    <tr>\n","      <th>1185657</th>\n","      <td>1</td>\n","      <td>1982819007</td>\n","      <td>Sun May 31 11:57:58 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>inmollywood</td>\n","      <td>@freshalicious heehee  I felt dumb though b/c ...</td>\n","    </tr>\n","    <tr>\n","      <th>1164625</th>\n","      <td>1</td>\n","      <td>1979774158</td>\n","      <td>Sun May 31 04:27:08 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>Ginababy127</td>\n","      <td>@IrishLad585 you r right. Thanx</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         sentiment_score  ...                                              tweet\n","3885                   0  ...  ooooooooooooh my headddd  uncle johnny i never...\n","12255                  0  ...  what a shame! you can't type persian &amp; rea...\n","133516                 0  ...  @rugzdbewler hardly working lol. I think I got...\n","1185657                1  ...  @freshalicious heehee  I felt dumb though b/c ...\n","1164625                1  ...                   @IrishLad585 you r right. Thanx \n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tvVAgVXkoZ1v","executionInfo":{"status":"ok","timestamp":1619491783439,"user_tz":240,"elapsed":5720,"user":{"displayName":"Lastk7","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz2ulFWgLLy_9S6Hxq2-LMJAKNqCtmNEvz6Hp3=s64","userId":"16520756898672502912"}},"outputId":"e93efef1-6f7e-490d-aa50-2c891724b4b7"},"source":["# download the glove embedding\n","!wget https://raw.githubusercontent.com/aritter/aritter.github.io/master/files/glove.840B.300d.conll_filtered.txt\n","\n","def read_GloVe(filename):\n","  embeddings = {}\n","\n","  for line in open(filename).readlines():\n","    #print(line)\n","    fields = line.strip().split(\" \")\n","    word = fields[0]\n","    embeddings[word] = [float(x) for x in fields[1:]]\n","    \n","  return embeddings\n","\n","GloVe = read_GloVe(\"glove.840B.300d.conll_filtered.txt\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["--2021-04-27 02:49:37--  https://raw.githubusercontent.com/aritter/aritter.github.io/master/files/glove.840B.300d.conll_filtered.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 69798443 (67M) [text/plain]\n","Saving to: ‘glove.840B.300d.conll_filtered.txt’\n","\n","glove.840B.300d.con 100%[===================>]  66.56M   136MB/s    in 0.5s    \n","\n","2021-04-27 02:49:40 (136 MB/s) - ‘glove.840B.300d.conll_filtered.txt’ saved [69798443/69798443]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QGcroJif4j0l"},"source":["# Vocabulary"]},{"cell_type":"code","metadata":{"id":"RfnCGIi3N72G","executionInfo":{"status":"ok","timestamp":1619491873168,"user_tz":240,"elapsed":311,"user":{"displayName":"Lastk7","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz2ulFWgLLy_9S6Hxq2-LMJAKNqCtmNEvz6Hp3=s64","userId":"16520756898672502912"}}},"source":["unk_token = '<unk>'\n","pad_token = '<pad>'\n","bos_token = '<bos>'\n","eos_token = '<eos>'\n","\n","def build_vocab_from_counter(counter):\n","    return Vocab(counter, specials=[unk_token, pad_token, bos_token, eos_token])\n","\n","def read_counter(filename):\n","    counter = Counter()\n","    with open(filename, 'rb') as f:\n","        counter = pickle.load(f)\n","    return counter"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"_j0hec324b53","executionInfo":{"status":"ok","timestamp":1619491875316,"user_tz":240,"elapsed":876,"user":{"displayName":"Lastk7","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz2ulFWgLLy_9S6Hxq2-LMJAKNqCtmNEvz6Hp3=s64","userId":"16520756898672502912"}}},"source":["def build_vocab_from_df(data_df, GloVe, tokenizer, save_counter=True, save_filename='vocab_counter.pickle'):\n","  counter = Counter()\n","\n","  for idx, row in data_df.iterrows():\n","    counter.update(tokenizer(row['tweet']))\n","\n","  for word in GloVe.keys():\n","    counter.update(tokenizer(word))\n","  \n","  # save the counter content\n","  with open(save_filename, 'wb') as f:\n","    pickle.dump(counter, f)\n","    \n","  return Vocab(counter, specials=[unk_token, pad_token, bos_token, eos_token])\n","\n","basic_english_tokenizer = get_tokenizer('basic_english')\n","if args.read_vocab == True:\n","  counter = read_counter(DATA_FOLDER_PATH + 'vocab_counter.pickle')\n","  large_data_vocab = build_vocab_from_counter(counter)\n","else:\n","  large_data_vocab = build_vocab_from_df(large_data_df, GloVe, basic_english_tokenizer)\n","\n","PAD_IDX = large_data_vocab[pad_token]\n","BOS_IDX = large_data_vocab[bos_token]\n","EOS_IDX = large_data_vocab[eos_token]\n","UNK_IDX = large_data_vocab[unk_token]"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"s0UeF9PqYepp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619491876808,"user_tz":240,"elapsed":233,"user":{"displayName":"Lastk7","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz2ulFWgLLy_9S6Hxq2-LMJAKNqCtmNEvz6Hp3=s64","userId":"16520756898672502912"}},"outputId":"0a48d76c-1d68-4071-b238-bff632e9c657"},"source":["print(PAD_IDX)\n","print(EOS_IDX)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["1\n","3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qbX69UKvlM5C"},"source":["# Dataset & DataLoader"]},{"cell_type":"code","metadata":{"id":"q4t2H8iulg-A","executionInfo":{"status":"ok","timestamp":1619491926473,"user_tz":240,"elapsed":48302,"user":{"displayName":"Lastk7","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz2ulFWgLLy_9S6Hxq2-LMJAKNqCtmNEvz6Hp3=s64","userId":"16520756898672502912"}}},"source":["def get_data_from_df(data_df):\n","  data = []\n","\n","  for idx, row in data_df.iterrows():\n","    tokenized_text = torch.tensor([large_data_vocab[token] for token in basic_english_tokenizer(row['tweet'])])\n","    target_score = row['sentiment_score']\n","    data.append((tokenized_text, target_score))\n","    \n","  return data\n","\n","train_data_df, test_data_df = train_test_split(large_data_df, test_size=args.test_ratio)\n","train_data_df, val_data_df = train_test_split(train_data_df, test_size=args.val_ratio)\n","\n","train_data = get_data_from_df(train_data_df)\n","val_data = get_data_from_df(val_data_df)\n","test_data = get_data_from_df(test_data_df)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"sNSOYfv5Em8_","executionInfo":{"status":"ok","timestamp":1619491926794,"user_tz":240,"elapsed":317,"user":{"displayName":"Lastk7","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz2ulFWgLLy_9S6Hxq2-LMJAKNqCtmNEvz6Hp3=s64","userId":"16520756898672502912"}}},"source":["def generate_batch(raw_data):\n","  data_batch = []\n","  target_batch = []\n","\n","  for data, target in raw_data:\n","    data_batch.append(torch.cat([torch.tensor([BOS_IDX]), data, torch.tensor([EOS_IDX])]))\n","    target_batch.append(target)\n","    \n","  data_batch = pad_sequence(data_batch, padding_value=PAD_IDX, batch_first=args.batch_first)\n","  target_batch = torch.tensor(target_batch)\n","  return data_batch, target_batch\n","\n","train_dataloader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True, collate_fn=generate_batch)\n","val_dataloader = DataLoader(val_data, batch_size=args.batch_size, shuffle=True, collate_fn=generate_batch)\n","test_dataloader = DataLoader(test_data, batch_size=args.batch_size, shuffle=True, collate_fn=generate_batch)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oe6b-Ss5lRO4"},"source":["# Model Definition"]},{"cell_type":"code","metadata":{"id":"3Jb9UydNlhdk","executionInfo":{"status":"ok","timestamp":1619491926933,"user_tz":240,"elapsed":452,"user":{"displayName":"Lastk7","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz2ulFWgLLy_9S6Hxq2-LMJAKNqCtmNEvz6Hp3=s64","userId":"16520756898672502912"}}},"source":["class RNN(nn.Module):\n","  def __init__(self, input_dim=args.embedding_dim, hidden_dim=args.hidden_dim, \n","              num_out=args.output_dim):\n","    super().__init__()\n","    self.num_words = len(large_data_vocab) # TODO: need to generalize this\n","    self.embedding = nn.Embedding(self.num_words, input_dim)\n","    self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=args.batch_first)\n","    self.fc = nn.Linear(hidden_dim, num_out)\n","\n","  def init_glove(self, GloVe):\n","    for word in GloVe.keys():\n","      self.embedding.weight.data[large_data_vocab[word]] = torch.as_tensor(GloVe[word])\n","\n","  def forward(self, text):\n","    embedded = self.embedding(text)\n","    output, hidden = self.rnn(embedded)\n","    return self.fc(hidden.squeeze(0))"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"sbNX0Xluqzz0","executionInfo":{"status":"ok","timestamp":1619491926934,"user_tz":240,"elapsed":450,"user":{"displayName":"Lastk7","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz2ulFWgLLy_9S6Hxq2-LMJAKNqCtmNEvz6Hp3=s64","userId":"16520756898672502912"}}},"source":["class BaselineGRU(nn.Module):\n","  def __init__(self, input_dim=args.embedding_dim, hidden_dim=args.hidden_dim, \n","               num_out=args.output_dim, bidirectional=True, batch_first=args.batch_first):\n","    super().__init__()\n","    self.num_words = len(large_data_vocab) # TODO: need to generalize this\n","    self.input_dim = input_dim\n","    self.hidden_dim = hidden_dim\n","\n","    self.embedding = nn.Embedding(self.num_words, input_dim)\n","    self.gru = nn.GRU(input_dim, hidden_dim, bidirectional=bidirectional, batch_first=batch_first)\n","    self.linear = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, num_out)\n","\n","  def init_glove(self, GloVe):\n","    for word in GloVe.keys():\n","      self.embedding.weight.data[large_data_vocab[word]] = torch.as_tensor(GloVe[word])\n","\n","  def forward(self, x):\n","    if args.batch_first == True:\n","      lengths = torch.sum(x != PAD_IDX, dim=1).cpu()\n","      batch_size, max_sentence_length = x.shape\n","    else:\n","      lengths = torch.sum(x != PAD_IDX, dim=0).cpu()\n","      max_sentence_length, batch_size = x.shape\n","\n","    embed_x = self.embedding(x)\n","    embed_x = pack_padded_sequence(embed_x, lengths, enforce_sorted=False, batch_first=args.batch_first)\n","\n","    out, hidden = self.gru(embed_x)\n","    out, out_length = pad_packed_sequence(out, padding_value=PAD_IDX, batch_first=args.batch_first)\n","    hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n","    return self.linear(hidden)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"ATDJyZFaq0Ob","executionInfo":{"status":"ok","timestamp":1619491926934,"user_tz":240,"elapsed":447,"user":{"displayName":"Lastk7","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz2ulFWgLLy_9S6Hxq2-LMJAKNqCtmNEvz6Hp3=s64","userId":"16520756898672502912"}}},"source":["class BaselineLSTMwithDropout(nn.Module):\n","  def __init__(self, embedding_dim=args.embedding_dim, hidden_dim=args.hidden_dim, \n","                output_dim=args.output_dim, n_layers=args.num_layers, \n","                bidirectional=True, dropout=args.dropout):\n","    super().__init__()\n","    self.vocab_size = len(large_data_vocab)\n","    self.embedding = nn.Embedding(self.vocab_size, embedding_dim, padding_idx=PAD_IDX)\n","    self.lstm = nn.LSTM(embedding_dim, \n","                        hidden_dim, \n","                        num_layers=n_layers, \n","                        bidirectional=bidirectional, \n","                        dropout=dropout)\n","    self.fc = nn.Linear(hidden_dim * 2, output_dim)\n","    self.dropout = nn.Dropout(dropout)\n","\n","\n","  def init_glove(self, GloVe):\n","    for word in GloVe.keys():\n","      self.embedding.weight.data[large_data_vocab[word]] = torch.as_tensor(GloVe[word])\n","\n","  def forward(self, x):\n","    if args.batch_first == True:\n","      lengths = torch.sum(x != PAD_IDX, dim=1).cpu()\n","      batch_size, max_sentence_length = x.shape\n","    else:\n","      lengths = torch.sum(x != PAD_IDX, dim=0).cpu()\n","      max_sentence_length, batch_size = x.shape\n","\n","    embedded = self.dropout(self.embedding(x))\n","    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths.to('cpu'), enforce_sorted=False)\n","    packed_output, (hidden, cell) = self.lstm(packed_embedded)\n","    output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, padding_value=PAD_IDX)\n","    #output = [sent len, batch size, hid dim * num directions]    \n","    #hidden = [num layers * num directions, batch size, hid dim]\n","    #cell = [num layers * num directions, batch size, hid dim]\n","    hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n","    return self.fc(hidden)"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WljXEPADliuQ"},"source":["# Define Training Procedure"]},{"cell_type":"code","metadata":{"id":"7835Yj5uPV3d","executionInfo":{"status":"ok","timestamp":1619491926935,"user_tz":240,"elapsed":446,"user":{"displayName":"Lastk7","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz2ulFWgLLy_9S6Hxq2-LMJAKNqCtmNEvz6Hp3=s64","userId":"16520756898672502912"}}},"source":["def get_pretrained_model_path():\n","  return DATA_FOLDER_PATH + args.model_name + '.pt'\n","\n","\n","def get_model():\n","  if args.model_name == 'baseline_lstm_with_dropout':\n","    return BaselineLSTMwithDropout()\n","  elif args.model_name == 'rnn':\n","    return RNN()\n","  elif args.model_name == 'baseline_gru':\n","    return BaselineGRU()\n","\n","\n","def get_criterion():\n","  if args.criterion == 'crossEntropy':\n","    return nn.CrossEntropyLoss()\n","  else:\n","    raise Exception('The loss function you specified is invalid')\n","\n","\n","def get_optimizer(model):\n","  if args.optimizer == 'Adam':\n","    return optim.Adam(model.parameters(), lr=args.learning_rate)\n","  else:\n","    raise Exception('The optimizer you specified is invalid')"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"7l3lc-0IlmmE","executionInfo":{"status":"ok","timestamp":1619491926935,"user_tz":240,"elapsed":443,"user":{"displayName":"Lastk7","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz2ulFWgLLy_9S6Hxq2-LMJAKNqCtmNEvz6Hp3=s64","userId":"16520756898672502912"}}},"source":["def train(model, iterator, optimizer, criterion):\n","  model.train()\n","  epoch_loss = 0\n","  correct_pred_cnt = 0\n","  total_cnt = 0\n","\n","  for (x, y) in tqdm(iterator):\n","    x, y = x.to(args.device), y.to(args.device)\n","\n","    optimizer.zero_grad()\n","\n","    output = model(x)\n","    correct_pred_cnt += torch.sum((torch.argmax(output, 1) == y)).item()\n","    total_cnt += [*y.size()][0] # This is used to convert torch.Size to int type\n","\n","    loss = criterion(output, y)\n","    loss.backward()\n","\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), args.gradient_clip)\n","    optimizer.step()\n","\n","    epoch_loss += loss.item()\n","  \n","\n","  return epoch_loss / len(iterator), correct_pred_cnt * 1.0 / total_cnt\n","\n","\n","def evaluate(model, iterator, criterion):\n","  model.eval()\n","  epoch_loss = 0\n","  correct_pred_cnt = 0\n","  total_cnt = 0\n","\n","  with torch.no_grad():\n","    for (x, y) in tqdm(iterator):\n","        x, y = x.to(args.device), y.to(args.device)\n","\n","        output = model(x) \n","        correct_pred_cnt += torch.sum((torch.argmax(output, 1) == y)).item()\n","        total_cnt += [*y.size()][0]\n","\n","        loss = criterion(output, y)\n","        epoch_loss += loss.item()\n","\n","  return epoch_loss / len(iterator), correct_pred_cnt * 1.0 / total_cnt\n","\n","\n","def epoch_time(start_time, end_time):\n","  elapsed_time = end_time - start_time\n","  elapsed_mins = int(elapsed_time / 60)\n","  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","  return elapsed_mins, elapsed_secs"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BWg1nmS1lm_u"},"source":["# Train"]},{"cell_type":"code","metadata":{"id":"TgNLIK3ulo8D","executionInfo":{"status":"ok","timestamp":1619491937219,"user_tz":240,"elapsed":10715,"user":{"displayName":"Lastk7","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz2ulFWgLLy_9S6Hxq2-LMJAKNqCtmNEvz6Hp3=s64","userId":"16520756898672502912"}}},"source":["model = get_model().to(args.device)\n","\n","if args.load_pretrained_weight == True:\n","  pretrained_model_path = get_pretrained_model_path()\n","  model.load_state_dict(torch.load(pretrained_model_path))\n","else:\n","  best_val_loss = float('inf')\n","\n","  train_losses, val_losses = [], []\n","  train_accuracies, val_accuracies = [], []\n","\n","  model.init_glove(GloVe)\n","\n","  criterion = get_criterion()\n","  optimizer = get_optimizer(model)\n","\n","  for epoch in range(args.num_epoch):\n","      start_time = time.time()\n","\n","      train_loss, train_acc = train(model, train_dataloader, optimizer, criterion)\n","      val_loss, val_acc = evaluate(model, val_dataloader, criterion)\n","\n","      end_time = time.time()\n","      epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","      print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","      print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Train Accuarcy: {train_acc:.3f}')\n","      print(f'\\t Val. Loss: {val_loss:.3f} |  Val. PPL: {math.exp(val_loss):7.3f} | Val. Accuracy: {val_acc:.3f}')\n","\n","      train_losses.append(train_loss)\n","      val_losses.append(val_loss)\n","\n","      train_accuracies.append(train_acc)\n","      val_accuracies.append(val_acc)\n","\n","  test_loss, test_acc = evaluate(model, test_dataloader, criterion)\n","  print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} | Test Accuracy: {test_acc:.3f}')"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"YUNPQzXFOngD"},"source":["# download and save the model state\n","if args.save_pretrained_model == True:\n","  pretrained_model_path = get_pretrained_model_path()\n","  torch.save(model.state_dict(), pretrained_model_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lGgTbC8b0Wd2"},"source":["# Training Analysis & Inspection"]},{"cell_type":"code","metadata":{"id":"Nn0ngDZA0Vm6","colab":{"base_uri":"https://localhost:8080/","height":198},"executionInfo":{"status":"error","timestamp":1619491953087,"user_tz":240,"elapsed":226,"user":{"displayName":"Lastk7","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz2ulFWgLLy_9S6Hxq2-LMJAKNqCtmNEvz6Hp3=s64","userId":"16520756898672502912"}},"outputId":"e22ef705-2cbb-4354-d34c-a016e8eb094d"},"source":["def plot_result(train_accuracies, val_accuracies, train_losses, val_losses):\n","  epochs = np.arange(0, args.num_epoch)\n","  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n","  ax1.set_title('Accuracies against Epochs')\n","  ax1.set_xlabel('Epochs')\n","  ax1.set_ylabel('Accuracy')\n","  ax1.plot(epochs, train_accuracies, label='training accuracy')\n","  ax1.plot(epochs, val_accuracies, label='validation accuracy')\n","  ax1.legend()\n","\n","  ax2.set_title('Loss against Epochs')\n","  ax2.set_xlabel('Epochs')\n","  ax2.set_ylabel('Loss')\n","  ax2.plot(epochs, train_losses, label='training loss')\n","  ax2.plot(epochs, val_losses, label='validation loss')\n","  ax2.legend()\n","\n","plot_result(train_accuracies, val_accuracies, train_losses, val_losses)"],"execution_count":25,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-b9d00df813a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0max2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mplot_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'train_accuracies' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"Omdaiz_IlpZn"},"source":["# Inference"]},{"cell_type":"code","metadata":{"id":"EmiUdyjglqwI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619494198451,"user_tz":240,"elapsed":333,"user":{"displayName":"Lastk7","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Giz2ulFWgLLy_9S6Hxq2-LMJAKNqCtmNEvz6Hp3=s64","userId":"16520756898672502912"}},"outputId":"f39e6d07-dc2d-4d85-c537-9d7fa763262b"},"source":["def predict_sentiment(model, sentence):\n","    model.eval()\n","    tokenized_sentence = [token for token in basic_english_tokenizer(sentence)]\n","    indexed = [large_data_vocab[token] for token in tokenized_sentence]\n","    input = torch.LongTensor(indexed).to(args.device)\n","    input = input.unsqueeze(1)\n","    prediction = torch.sigmoid(model(input)).flatten().detach().cpu().numpy().tolist()\n","    formatted_prediction_str = 'Input: {} \\n P(negative) = {:.2f} \\n P(positive) = {:.2f} \\n\\n'. \\\n","                                format(sentence, prediction[0], prediction[1])\n","    return formatted_prediction_str\n","\n","print(predict_sentiment(model, 'this is a great movie'))\n","print(predict_sentiment(model, 'what an awful day'))\n","print(predict_sentiment(model, 'I work 40 hours a week for us to be this poor'))\n","print(predict_sentiment(model, 'Nice perfume.'))\n","print(predict_sentiment(model, 'Nice perfume. How long did you marinate in it?'))\n","print(predict_sentiment(model, 'I am hungry.'))\n","print(predict_sentiment(model, 'He wrote a meritorious theme about his visit.'))\n","print(predict_sentiment(model, 'She is friendly as a rattlesnack.'))\n","print(predict_sentiment(model, 'What is the weather today?'))"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Input: this is a great movie \n"," P(negative) = 0.20 \n"," P(positive) = 0.78 \n","\n","\n","Input: what an awful day \n"," P(negative) = 0.89 \n"," P(positive) = 0.11 \n","\n","\n","Input: I work 40 hours a week for us to be this poor \n"," P(negative) = 0.84 \n"," P(positive) = 0.20 \n","\n","\n","Input: Nice perfume. \n"," P(negative) = 0.26 \n"," P(positive) = 0.73 \n","\n","\n","Input: Nice perfume. How long did you marinate in it? \n"," P(negative) = 0.67 \n"," P(positive) = 0.34 \n","\n","\n","Input: I am hungry. \n"," P(negative) = 0.60 \n"," P(positive) = 0.42 \n","\n","\n","Input: He wrote a meritorious theme about his visit. \n"," P(negative) = 0.10 \n"," P(positive) = 0.89 \n","\n","\n","Input: She is friendly as a rattlesnack. \n"," P(negative) = 0.52 \n"," P(positive) = 0.48 \n","\n","\n","Input: What is the weather today? \n"," P(negative) = 0.63 \n"," P(positive) = 0.38 \n","\n","\n"],"name":"stdout"}]}]}